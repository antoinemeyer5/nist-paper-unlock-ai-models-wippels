\section{Results}
\label{sec:results}

Access new repositories give access to a lot of new models usable in Web Image
Processing Pipeline (WIPP). It also saves hours of computational time and
resources, as well as improves reusability of pre-trained AI models.

In the context of Image Analysis in WIPP, we are looking to use Mask
Generation Models.

\begin{table}[H]
\centering
\caption{\label{tab:number_of_newly_available_models}%
  Number of newly available Mask Generation models
}
\begin{tabular}{lc}
  \toprule
  Repository & Mask Generation Model(s) \\
  \midrule
  WIPP & 1 \\
  Hugging Face & 176 \\
  BioImage.IO & 32 \\
  Cellpose & 21 \\
  SAM2 & 8 \\
  \bottomrule
\end{tabular}
\end{table}

The level of documentation varies greatly from model to model. We have set up an
automatic documentation system for models trained within WIPP.

Finally to avoid lower confidence in external models, we have developed a way of
evaluating the results and therefore the relevance of the models tested, in
order to select and retain only those that deliver the best results.

\subsection{WIPP compute with 3D-RPE data}

Masks used are here
https://wipp-dev.nist.gov/images-collection/671abb516103440e64a77397 and images
used are here
/mnt/isgnas/project/csmet/bio/Proj-028-RPE-Measurements/sample\_data\_2024-10-24.
Therer are 162 images. The WIPP server specifications are \TODO\

\begin{table}[H]
\tiny
\centering
\begin{tabular}{llcc}
  \toprule
  Repository & Model Name & Inference Time per Image (s) & Accuracy (\%) \\
  \midrule
  WIPP & WIPP UNet CNN 1.0.0 & \TODO\ &  \\
  Hugging Face & facebook/sam-vit-huge & 7.04 & $\SI{40.62}{\percent} \pm \SI{34.99}{\percent}$ \\
               & Zigeng/SlimSAM-uniform-50 & 5.79 & $\SI{25.05}{\percent} \pm \SI{22.02}{\percent}$ \\
               & jadechoghari/robustsam-vit-large & 6.76 & $\SI{38.19}{\percent} \pm \SI{35.67}{\percent}$ \\
  BioImage.IO & 10.5281/zenodo.5869899 & Issue with Docker & Need help \\
              & 10.5281/zenodo.5764892 & Same reason & / \\
  Cellpose & cyto3 & \TODO\ &  \\
           & nuclei & \TODO\ &  \\
  SAM2 & facebook/sam2.1-hiera-large & 2.12 & $\SI{38.16}{\percent} \pm \SI{35.02}{\percent}$ \\
       & facebook/sam2-hiera-small & \TODO\ &  \\
  \bottomrule
\end{tabular}
\end{table}

\TODO\ Bad results because 3D data with 2D models so a lot of allusionations or
detect things that should not be detected. But otherwise, on certain images,
results are great!! Do we have to select specific data?

\subsection{Local compute with RPEimplants data}

Data used are from this link https://isg.nist.gov/deepzoomweb/data/RPEimplants.
They contains images of 2D Measurement of Retinal Pigment Epithelium Function
Using Quantitative Bright-Field Microscopy. There are 1032 images but for this
results we compute on only 210 images. The local computer specifications are one
GPU Quadro RTX 4000 with CUDA version 12.7 and Memory 8192 MiB.

\begin{table}[H]
\tiny
\centering
\begin{tabular}{llcc}
  \toprule
  Repository & Model Name & Inf. Time per Image (s) & Accuracy (\%) \\
  \midrule
  Hugging Face & facebook/sam-vit-huge & 4.16 & $\SI{85.87}{\percent} \pm \SI{3.98}{\percent}$ \\
                & Zigeng/SlimSAM-uniform-50 & 2.81 & $\SI{79.78}{\percent} \pm \SI{5.31}{\percent}$ \\
                & jadechoghari/robustsam-vit-large & 3.57 & $\SI{83.08}{\percent} \pm \SI{3.34}{\percent}$ \\
  BioImage.IO & 10.5281/zenodo.5869899 & 0.31 & $\SI{89.30}{\percent} \pm \SI{0.84}{\percent}$ \\
              & 10.5281/zenodo.5764892 & 0.38 & $\SI{10.44}{\percent} \pm \SI{3.20}{\percent}$ \\
  Cellpose & cyto3 & Unqualified (M.G. model) & with segme. mask data.  \\
            & nuclei & Same reason & / \\
  \bottomrule
\end{tabular}
\end{table}

---

Even if the results may not be perfect, this method allows you to quickly try
out a new model at reduced cost. It is also easy to change dataset. We can then
take a promising model and improve it by finetuning it. We hope to improve the
speed of data analysis within WIPP and enable better overall results.
